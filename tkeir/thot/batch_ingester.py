# -*- coding: utf-8 -*-
"""Convert source document to tkeir indexer document
Author : Eric Blaudez (Eric Blaudez)

Copyright (c) 2022 THALES 
All Rights Reserved.
"""
import os
import sys
import argparse
import requests
import traceback
import base64
import json
import hashlib
import csv
from joblib import Parallel, delayed
from tqdm import tqdm
import time
from uuid import uuid4


dir_path = os.path.dirname(os.path.realpath(__file__))
sys.path.insert(0, os.path.abspath(os.path.join(dir_path, "../../")))
sys.path.insert(0, os.path.abspath(os.path.join(dir_path, "../")))
sys.path.insert(0, os.path.abspath(os.path.join(dir_path, "./")))

from thot import __author__, __copyright__, __credits__, __maintainer__, __email__, __status__
import thot.core.Constants as Constants
from thot.core.ThotLogger import ThotLogger, LogUserContext
import thot.core.Constants as Constants
from thot.core.Utils import timelimit
from thot.tasks.converters.ConverterConfiguration import ConverterConfiguration
from thot.tasks.converters.Converter import Converter
from thot.tasks.tokenizer.TokenizerConfiguration import TokenizerConfiguration
from thot.tasks.tokenizer.Tokenizer import Tokenizer
from thot.tasks.morphosyntax.MorphoSyntacticTaggerConfiguration import MorphoSyntacticTaggerConfiguration
from thot.tasks.morphosyntax.MorphoSyntacticTagger import MorphoSyntacticTagger
from thot.tasks.ner.NERTaggerConfiguration import NERTaggerConfiguration
from thot.tasks.ner.NERTagger import NERTagger
from thot.tasks.syntax.SyntacticTaggerConfiguration import SyntacticTaggerConfiguration
from thot.tasks.syntax.SyntacticTagger import SyntacticTagger
from thot.tasks.keywords.KeywordsConfiguration import KeywordsConfiguration
from thot.tasks.keywords.KeywordsExtractor import KeywordsExtractor
from thot.tasks.relations.RelationClusterizerConfiguration import RelationClusterizerConfiguration
from thot.tasks.relations.ClusterInference import ClusteringInference
from thot.tasks.document_classification.ZeroShotClassificationConfiguration import ZeroShotClassificationConfiguration
from thot.tasks.document_classification.ZeroShotClassification import ZeroShotClassification
from thot.tasks.sentiment.SentimentAnalysisConfiguration import SentimentAnalysisConfiguration
from thot.tasks.sentiment.SentimentAnalysis import SentimentAnalysis
from thot.tasks.summarizer.SummarizerConfiguration import SummarizerConfiguration
from thot.tasks.summarizer.Summarizer import Summarizer
from thot.tasks.indexing.IndexingConfiguration import IndexingConfiguration
from thot.tasks.indexing.Indexing import Indexing
from thot.tasks.indexing.IndicesManager import IndicesManager
from thot.tasks.taggers_pipeline.TaggersPipelineConfiguration import TaggersPipelineConfiguration
from thot.tasks.taggers_pipeline.TaggersPipeline import TaggersPipeline 
from thot.relation_clustering import cluster_train


task_auto_model = {
        "converter": {"svcConfig": ConverterConfiguration, "service": Converter},
        "tokenizer": {"svcConfig": TokenizerConfiguration, "service": Tokenizer},
        "morphosyntax": {"svcConfig": MorphoSyntacticTaggerConfiguration, "service": MorphoSyntacticTagger},
        "ner": {"svcConfig": NERTaggerConfiguration, "service": NERTagger},
        "syntax": {"svcConfig": SyntacticTaggerConfiguration, "service": SyntacticTagger},
        "keywords": {"svcConfig": KeywordsConfiguration, "service": KeywordsExtractor},
        "clusterinfer": {"svcConfig": RelationClusterizerConfiguration, "service": ClusteringInference},
        "zeroshotclassifier": {"svcConfig": ZeroShotClassificationConfiguration, "service": ZeroShotClassification},
        "sentiment": {"svcConfig": SentimentAnalysisConfiguration, "service": SentimentAnalysis},
        "summarizer": {"svcConfig": SummarizerConfiguration, "service": Summarizer},
        "index": {"svcConfig": IndexingConfiguration, "service": Indexing},
    }

def ingest_data(config, input,select=None):
    all_tasks=set(["converter","tokenizer","morphosyntax","ner","syntax","keywords","clusterinfer","zeroshotclassifier","sentiment","summarizer","index"])
    if select != None:
        all_tasks=set(select.split(","))
    os.environ["TOKENIZERS_PARALLELISM"] = "true"
    pipelineConfiguration = TaggersPipelineConfiguration()
    cid = "autogenerated-" + str(uuid4())
    log_context = LogUserContext(cid)
    with open(config) as fh:
        pipelineConfiguration.load(fh)
        fh.close()
    ThotLogger.loads(pipelineConfiguration.logger_config.configuration)
    ThotLogger.info("Process chain on :"+ input,context=log_context)
   
    pipeline = TaggersPipeline(config=pipelineConfiguration)
    
    fileid_list = []
    for (dirpath, dirnames, filenames) in os.walk(input):
        for filename in filenames:            
            fileid_list.append(os.path.join(dirpath,filename))
    settingsConfig = pipeline.config.configuration["settings"]
    for task in pipeline.config.configuration["tasks"]:
        if task["task"] in all_tasks:
            svcConfig = pipeline._load_config(task)
            if task["task"] == "converter":
                ThotLogger.warning("Converter cannot be applied, batch ingester takes only tkeir format documents !",context=log_context)
                continue
            if task["task"] == "index":
                try:
                    IndicesManager.createIndices(config=svcConfig.configuration)
                except Exception as e:
                    tracebck = traceback.format_exc()
                    ThotLogger.error(
                        "Exception:" + str(e) + " | trace:",
                        trace=Constants.exception_error_and_trace(str(e), str(traceback.format_exc())),
                        context=log_context,
                    )
            if task["task"] == "clusterinfer":
                if not os.path.isfile(svcConfig.configuration["clustering-model"]["semantic-quantizer-model"]):
                    #train clustering model
                    cluster_train(os.path.join(task["resources-base-path"], task["configuration"]),
                                input,
                                os.path.dirname(svcConfig.configuration["clustering-model"]["semantic-quantizer-model"]),
                                None)
                serviceInstance = task_auto_model[task["task"]]["service"](svcConfig, embeddings_server=False)
            else:
                serviceInstance = task_auto_model[task["task"]]["service"](svcConfig)
            input_files = tqdm(fileid_list)
            ThotLogger.info("Run task:"+task["task"],context=log_context)
            for filename in input_files:
                with open(filename, encoding="utf-8") as json_f:
                    file_loaded = True
                    try:
                        json_request = json.load(json_f)
                    except Exception as e:
                        ThotLogger.error(
                            "Cannot load " + filename,
                            trace=Constants.exception_error_and_trace(str(e), str(traceback.format_exc())),
                            context=log_context,
                        )
                        file_loaded = False
                    json_f.close()
                    result = None
                    try:
                        if file_loaded:
                            if settingsConfig["max-time-per-task"] > 0:
                                result = timelimit(settingsConfig["max-time-per-task"], serviceInstance.run, (json_request,))
                            else:
                                result = serviceInstance.run(json_request)
                    except Exception as e:
                        ThotLogger.error(
                            "Error occured on task:" + task["task"],
                            trace=Constants.exception_error_and_trace(str(e), str(traceback.format_exc())),
                            context=log_context,
                        )
                    if result:
                        with open(filename, "w", encoding="utf-8") as output_f:
                            json.dump(result, output_f, indent=2, ensure_ascii=False)
                            output_f.close()
                        if task["save-output"]:
                            bname = os.path.join(task["output-dir"], os.path.basename(filename))
                            ThotLogger.info("Save:" + bname, context=log_context)
                            with open(bname, "w", encoding="utf-8") as output_f:
                                json.dump(result, output_f, indent=2, ensure_ascii=False)
                                output_f.close()
                    else:
                        ThotLogger.error("Error on task:"+task["task"]+" for file '"+filename+"'",context=log_context)
        else:
            ThotLogger.info("Task:"+task["task"]+" ignored",context=log_context)
    
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-c", "--config", default=None, type=str, help="pipeline configuration file")
    parser.add_argument("-i", "--input", default=None, help="absolute path of input directory containing tkeir files")
    parser.add_argument("-o", "--output", default=None, help="absolute path of output directory")
    parser.add_argument("-s","--select",default=None,help="list of tasks (comma separated) in [converter,tokenizer,morphosyntax,ner,syntax,keywords,clusterinfer,zeroshotclassifier,sentiment,summarizer,index]")
    args = parser.parse_args()
    ingest_data(args.config,args.input,args.select)
    
        
if __name__ == "__main__":    
    main()
